services:
  vllm-app:
    build: .
    container_name: vllm-app
    working_dir: /app
    ports:
      - "8501:8501"
    environment:
      # Core app config
      PIPER_BIN: /usr/local/bin/piper
      PIPER_VOICE: /opt/piper/voices/en_US-amy-medium.onnx
      WAV2LIP_CHECKPOINT: /models/wav2lip/wav2lip_gan.pth
      PYTHONHTTPSVERIFY: "0"
      ORT_LOG_SEVERITY_LEVEL: "3"
      FAL_API_KEY: "19fff532-18a9-471b-b754-0b3f93855652:1a58841a03aa4c77486ed9acbe3214e2"
      FAL_VOICE: "Bill"
      FAL_INF_ENDPOINT: "fal-ai/infinitalk/single-text"
      FAL_QUEUE_BASE: "https://queue.fal.run"
      D_ID_API_KEY: "ZWppdHVuZGUzQGdtYWlsLmNvbQ:Kg_E2A2C8K7tpZZaq9MDt"
      DID_TEXT: "Hello, this is a test message."
    volumes:
      - ./app:/app/app
      - ./models:/models
      - ./voices:/opt/piper/voices
      - ./outputs:/app/outputs
    restart: unless-stopped